{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2caae95a-19ed-48e5-8316-4c7326072f16",
   "metadata": {},
   "source": [
    "# Semantic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af33e61a-71bb-47b0-b867-441751e0134b",
   "metadata": {},
   "source": [
    "## What is semantic search?\n",
    "Semantic search is a data searching technique that focuses on understanding the contextual meaning and intent behind a user's query, rather than only matching keywords.\n",
    "\n",
    "Traditional search engines typically focus on matching keywords within a search query to corresponding keywords in indexed web pages. In contrast, semantic search aims to comprehend the deeper meaning and intent behind a user's search, much like a human would."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67904e32-2cd9-4eed-8050-18d547da22f3",
   "metadata": {},
   "source": [
    "## Semantic Meaning and Similarity Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1e9ff2-6b16-4502-bc60-e3251b13ebad",
   "metadata": {},
   "source": [
    "**Semantic meaning** refers to the contextual or conceptual similarity between pieces of information. In AI systems, capturing semantic meaning allows models to understand not just the literal text but the underlying concepts and realtionships between words or phrases.\n",
    "\n",
    "**Similarity search** is the process of finding items in a dataset that are most similar to a given query. Instead of relying solely on exact matches, similarity search considers the semantic meaning, enabling more accurate and relevant results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fca946d-f06f-4c05-b782-61a5dd18b532",
   "metadata": {},
   "source": [
    "### How do AI systems represent meaning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b1ca5d-e72e-474a-a4fd-ea6e15888d28",
   "metadata": {},
   "source": [
    "**Vector Embeddings:** AI models convert text, images, or other data types into numerical representations called *vectors*. These vectors capture relationships between words, sentences, or documents by encoding semantic information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8487893c-22b6-4d34-9c60-0c3a7400e069",
   "metadata": {},
   "source": [
    "**Semantic Vector Space:** Vectors reside in a high-dimensional space where their postions determine how similar or different they are.\n",
    "\n",
    "Similar items are closer together, while dissimlar items are farther apart."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14bf2cbb-5a73-4d61-bd2e-a33de2f35fe2",
   "metadata": {},
   "source": [
    "![](./resources/embedding_img.webp)\n",
    "\n",
    "*Rozado, David (2020). Word embeddings map words in a corpus of text to vector space. PLOS ONE. Figure. [https://doi.org/10.1371/journal.pone.0231189.g008](https://doi.org/10.1371/journal.pone.0231189.g008)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87d8003-eee3-43d7-99f7-6656f6ab9646",
   "metadata": {},
   "source": [
    "#### Example of Semantic Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b5f966-b68b-4287-9bab-02519ecbcd76",
   "metadata": {},
   "source": [
    "Consider the words **\"Cat\"** and **\"kitten\"**. These words are semantically similar because they relate to the same animal at different life stages. In vector, space, their embeddings might look something like:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b555e65d-37a0-4b2d-8dbf-c03fe261926b",
   "metadata": {},
   "source": [
    "cat = ```[1.5, -0.4, 7.2, 19.6, 3.1, ..., 20.2]```\n",
    "\n",
    "kitten = ```[1.5, -0.4, 7.2, 19.5, 3.2, ..., 20.8]```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be99125c-4791-4262-96c3-6ae28e775ad0",
   "metadata": {},
   "source": [
    "Notice that their embeddings have similar numbers, especially at the beginning of the dimensions, with minor changes towards the end. This closeness in vector space reflects their semantic similarity.\n",
    "\n",
    "In contrast, words like **\"dog\"** or **\"apple\"** would have embeddings that are significantly different, indicating less semantic similarity as compared to \"cat\" or \"kitten\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6385de-dd3e-441f-ba0b-febd07998a90",
   "metadata": {},
   "source": [
    "### Applications of Semantic Meaning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3db3b74-8bb5-4e1e-975e-43fee4ee6520",
   "metadata": {},
   "source": [
    "**Semantic Search Engines:** Improve search results by understanding the context and meaning behind queries.\n",
    "\n",
    "**Recommendation Systems**: Suggest items similar to a user's preferences based on semantic similarities.\n",
    "\n",
    "**Document Clustering and Classification:** Group documents with similar content, aiding in organziation and rerieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99662209-2e5a-4ade-a085-830cb53bb1ce",
   "metadata": {},
   "source": [
    "## What is Similarity Search?\n",
    "\n",
    "**Similarity search** involves finding items that are most similar to a given query based on their vector representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668cb292-ad1a-4784-a8a1-b9cc9e71377e",
   "metadata": {},
   "source": [
    "### How it Works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cddf482-e2c7-48d1-8710-98dfe8666988",
   "metadata": {},
   "source": [
    "1. **Vector Representation:** Each item is represented as a vector in a multi-dimensional space.\n",
    "\n",
    "2. **Measuring Similarity:** Mathematical functions calculate how close these vectors are to each other.\n",
    "\n",
    "3. **Nearest Neighbor Search:** By calculating distances or angles between vectors, we identify the items most similar to our query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f9780b-f331-437b-93e7-7b15bf0cb33c",
   "metadata": {},
   "source": [
    "### Importance of Similarty Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d3ca3c-2b1c-4342-b52a-f2a1fff625bf",
   "metadata": {},
   "source": [
    "**Enhanced Search Capabilities:** Allows for more accurate and relevant search results by considering context and meaning.\n",
    "\n",
    "**Personalization:** Powers recommendation engines that tailor suggestions to individual users.\n",
    "\n",
    "**Clustering and Classification:** Facilities grouping similar data points, crucial in data analysis and pattern recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbccbd9-8556-4784-83ff-0aee3829f201",
   "metadata": {},
   "source": [
    "### Techniques for Measuring Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562cecbb-cd1f-4a3c-865b-dcf8085295f6",
   "metadata": {},
   "source": [
    "**Cosine Similarity:** Measures the angle between two vectors.\n",
    "\n",
    "**Euclidean Distance:** Measures the straight-line distance between two vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed1adf4-eac0-4833-8962-0231358d5225",
   "metadata": {},
   "source": [
    "## What is Cosine Similarity?\n",
    "\n",
    "Cosine similarity measures the similarity between two non-zero vectors by calculating the cosine of the angle between them. It is widely used in machine learning and data analysis, especially in text analysis, document comparison, search queries, and recommendation systems.\n",
    "\n",
    "- Similarty measure calculates the distances between data objects based on their feature dimensions in a dataset.\n",
    "- A smaller distance indicates a higher similarity, while a larger distance indicates a lower similarity.\n",
    "\n",
    "Consine similarity is the cosine of the angle between the vectorsl that is, it is the dot product of the vectors divided by the product of their lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f5dca7-2611-4ea6-9139-a8807c4794c7",
   "metadata": {},
   "source": [
    "![](./resources/cosine_sim.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d7a94f-27d6-455c-b849-6d5c2a1e6e3c",
   "metadata": {},
   "source": [
    "- **A*b**: Product of vectors A and B.\n",
    "- **||A||** and **||B||**: Magnitudes (lengths) of vectors A and B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529eb44f-34c0-4728-aa36-ceaa56109cf2",
   "metadata": {},
   "source": [
    "### Interpreting Values\n",
    "- **1**: Vectors are identical in direction.\n",
    "- **0**: Vectors are orthogonal (unrelated).\n",
    "- **-1**: Vectors are diametrcally opposite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b272b772-00e0-467e-8894-74164c9a9173",
   "metadata": {},
   "source": [
    "### Why use Cosine Similarity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd317952-b49e-4450-8417-6f53b42527aa",
   "metadata": {},
   "source": [
    "- **Focus on Direction:** Emphasizes the orientation of vectors rather than their magnitude, making it ideal for tet data where word counts may vary.\n",
    "- **Semantic Relevance:** Captures semantic similarity by considering the context and meaning of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82d29fc-75c9-435e-afe2-e0f91c26fd73",
   "metadata": {},
   "source": [
    "## What is Euclidean Distance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63427de-e919-4ebd-bc90-e14334630054",
   "metadata": {},
   "source": [
    "**Euclidean Distance** calculates the root of the sum of squared differences between corresponding components of two vectors. It measures how far apart two vectors are in space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c13011-ba6f-41a5-ad88-f98c2635843e",
   "metadata": {},
   "source": [
    "### Why Use Euclidean Distance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b48143e-3ef0-4e0b-bcc6-5e0e3230b5a8",
   "metadata": {},
   "source": [
    "- **Simplicity and Intuitiveness:** Easy to understand and compute.\n",
    "- **Effective for Numerical and Spatial Data:** Ideal for applications where absolute differences matter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d19900c-4367-42d7-b1cc-02775e0f2b4c",
   "metadata": {},
   "source": [
    "### Limitations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e85102-9ed0-4d52-85c0-36c4990d447a",
   "metadata": {},
   "source": [
    "- **Curse of Dimensionality:** In high-dimensional spaces, Euclidean Distance can become less meaningful due to data sparsity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c30994-0abc-4bc3-9165-7694770963b7",
   "metadata": {},
   "source": [
    "## Choosing Between Cosine Similarity and Euclidean Distance:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a792376-e5ab-4633-b852-261e89abe786",
   "metadata": {},
   "source": [
    "1. **Cosine Similarity** focuses on semantic meaning and direction, making it ideal for NLP tasks and text analysis.\n",
    "2. **Euclidean Distance** measures absolute closeness, excelling in numerical or spatial data applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7839997-0d21-49cf-8e71-32d96c1b0de6",
   "metadata": {},
   "source": [
    "## Implementing Cosine Similarity Search in (semantic search) in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bf223c4-f206-4aa9-b98c-772c26502089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Load the data source that you will be doing semantic searching on\n",
    "path='./resources/stock_news.json'\n",
    "\n",
    "with open(path,'r') as f:\n",
    "    loaded_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c41b61f-0c95-4121-b62c-c93b73d72ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in ./ai_libraries_env/lib/python3.13/site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy in ./ai_libraries_env/lib/python3.13/site-packages (2.3.2)\n",
      "Requirement already satisfied: pandas in ./ai_libraries_env/lib/python3.13/site-packages (2.3.1)\n",
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: openai in ./ai_libraries_env/lib/python3.13/site-packages (1.99.9)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./ai_libraries_env/lib/python3.13/site-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./ai_libraries_env/lib/python3.13/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./ai_libraries_env/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./ai_libraries_env/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./ai_libraries_env/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./ai_libraries_env/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./ai_libraries_env/lib/python3.13/site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./ai_libraries_env/lib/python3.13/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./ai_libraries_env/lib/python3.13/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./ai_libraries_env/lib/python3.13/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./ai_libraries_env/lib/python3.13/site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in ./ai_libraries_env/lib/python3.13/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./ai_libraries_env/lib/python3.13/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./ai_libraries_env/lib/python3.13/site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in ./ai_libraries_env/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in ./ai_libraries_env/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in ./ai_libraries_env/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./ai_libraries_env/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./ai_libraries_env/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./ai_libraries_env/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./ai_libraries_env/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in ./ai_libraries_env/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.1.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn numpy pandas python-dotenv openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972e35e8-ab92-4f2c-8e23-cb7b546f8008",
   "metadata": {},
   "source": [
    "After loading the json data, we need to generate embeddings for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93ecfa83-ee32-4ad4-bf5e-0577817ce15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n",
    "\n",
    "for ticker, articles in loaded_data.items():\n",
    "    for article in articles:\n",
    "        embedding_res = client.embeddings.create(model=EMBEDDING_MODEL, input=article.get(\"full_text\"))\n",
    "        embedding = embedding_res.data[0].embedding\n",
    "        article[\"embedding\"] = embedding\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940aac82-a230-4cb1-ba3f-223d93ec0701",
   "metadata": {},
   "source": [
    "Create a dataframe with the input data + the embeddings (this will act as our matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "068c3239-851f-4f7d-8ab7-8533b49d1a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame:\n",
      "     ticker                                              title  \\\n",
      "0     AAPL  Apple Inc. (AAPL) Teases New Product Launch wi...   \n",
      "1     AAPL  Apple's Launch of iPhone SE4 Not Seen Impactin...   \n",
      "2     AAPL  Apple Supplier Foxconn's Efforts To Make iPhon...   \n",
      "3     AAPL  Is Apple Inc. (AAPL) the Most Profitable Tech ...   \n",
      "4     AAPL  Apple Partners With Alibaba, Eyes Baidu for AI...   \n",
      "..     ...                                                ...   \n",
      "133    IBM  Corelight Cuts SIEM Ingest By Up to 80% withou...   \n",
      "134    IBM  The Zacks Analyst Blog Highlights Apple, Eli L...   \n",
      "135    IBM  Putting People First Drives Higher Adoption of...   \n",
      "136    IBM  Cohesity Appoints Carol Carpenter as Chief Mar...   \n",
      "137    IBM  Rashida Hodge of Microsoft and Gerben Bakker o...   \n",
      "\n",
      "                                                  link  \\\n",
      "0    https://finance.yahoo.com/news/apple-inc-aapl-...   \n",
      "1    https://finance.yahoo.com/news/apple-apos-laun...   \n",
      "2    https://finance.yahoo.com/news/apple-supplier-...   \n",
      "3    https://finance.yahoo.com/news/apple-inc-aapl-...   \n",
      "4    https://finance.yahoo.com/news/apple-partners-...   \n",
      "..                                                 ...   \n",
      "133  https://finance.yahoo.com/news/corelight-cuts-...   \n",
      "134  https://finance.yahoo.com/news/zacks-analyst-b...   \n",
      "135  https://finance.yahoo.com/news/putting-people-...   \n",
      "136  https://finance.yahoo.com/news/cohesity-appoin...   \n",
      "137  https://finance.yahoo.com/news/rashida-hodge-m...   \n",
      "\n",
      "                                             full_text  \\\n",
      "0    We recently compiled a list of the 10 High-Fly...   \n",
      "1    Apple (AAPL)'s launch of the fourth generation...   \n",
      "2    Apple Supplier Foxconn's Efforts To Make iPhon...   \n",
      "3    We recently published a list of 10 Most Profit...   \n",
      "4    Apple (AAPL, Financials) partnered with Alibab...   \n",
      "..                                                 ...   \n",
      "133  New data aggregation capability delivers neede...   \n",
      "134  For Immediate Release Chicago, IL – February 1...   \n",
      "135  Accelerating development of new tools in archi...   \n",
      "136  Carol Carpenter, Chief Marketing Officer, Cohe...   \n",
      "137  MILWAUKEE, Feb. 18, 2025 /PRNewswire/ -- Regal...   \n",
      "\n",
      "                                             embedding  \n",
      "0    [-0.0447944700717926, -0.035055339336395264, -...  \n",
      "1    [-0.051633723080158234, 0.00394114013761282, -...  \n",
      "2    [-0.03661007434129715, -0.0050949580036103725,...  \n",
      "3    [-0.02098972536623478, -0.02645847760140896, -...  \n",
      "4    [-0.02767382748425007, 0.00771879218518734, -0...  \n",
      "..                                                 ...  \n",
      "133  [0.006215594243258238, -0.0380832701921463, -0...  \n",
      "134  [-0.019215162843465805, -0.015028407797217369,...  \n",
      "135  [-0.014260932803153992, -0.0008599242428317666...  \n",
      "136  [-0.013134746812283993, -0.040588706731796265,...  \n",
      "137  [0.0004464929806999862, -0.02187563292682171, ...  \n",
      "\n",
      "[138 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataframe_rows = []\n",
    "# Loaded data items now have an embedding key-value pair\n",
    "for ticker, articles in loaded_data.items():\n",
    "    for article in articles:\n",
    "        dataframe_rows.append({\n",
    "            \"ticker\": article.get('ticker'),\n",
    "            \"title\": article.get('title'),\n",
    "            \"link\": article.get('link'),\n",
    "            \"full_text\": article.get('full_text'),\n",
    "            \"embedding\": article.get('embedding')\n",
    "        })\n",
    "\n",
    "df= pd.DataFrame(dataframe_rows)\n",
    "\n",
    "print(f'\\nDataFrame:\\n {df}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be7cfbf-019b-402b-a891-db8b6d0afdd7",
   "metadata": {},
   "source": [
    "Get a query to search for, and generate embeddings for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be2f2959-5d48-4575-b77e-76e67c91d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_search_query = \"What new product will Apple be releasing?\"\n",
    "user_embedding_res = client.embeddings.create(model=EMBEDDING_MODEL, input=user_search_query)\n",
    "user_in_embedding = user_embedding_res.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209d9a7b-7190-408a-a8b7-94e3b3723b32",
   "metadata": {},
   "source": [
    "Perform Cosine Similarity on the dataframe matrix with embeddings and the embeded user query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ba20d81-fbd6-41cb-8628-37bff44dc4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding Matrix created from dataframe:\n",
      "[[-0.04479447 -0.03505534 -0.00366255 ... -0.01124704 -0.02408498\n",
      "  -0.00260079]\n",
      " [-0.05163372  0.00394114 -0.00725202 ... -0.00103614 -0.01689744\n",
      "   0.01646663]\n",
      " [-0.03661007 -0.00509496 -0.00808166 ... -0.01099306 -0.00697733\n",
      "   0.00031634]\n",
      " ...\n",
      " [-0.01426093 -0.00085992 -0.02583767 ... -0.03460239 -0.01368575\n",
      "  -0.01221583]\n",
      " [-0.01313475 -0.04058871 -0.01504347 ... -0.0174124  -0.00266882\n",
      "  -0.01824228]\n",
      " [ 0.00044649 -0.02187563 -0.01777496 ... -0.01740364  0.00766454\n",
      "   0.00204933]]\n",
      "\n",
      "Original User input query embedding:\n",
      " [-0.07105432  0.00581561 -0.01986648  0.03576227 -0.03053769  0.01336187\n",
      " -0.01118714 -0.01256512 -0.03239241 -0.01304839]\n",
      "\n",
      "2D vector representation of the user input query embedding:\n",
      "[[-0.07105432  0.00581561 -0.01986648 ... -0.01256512 -0.03239241\n",
      "  -0.01304839]]\n",
      "Cosine Similarity results:\n",
      "     ticker                                              title  \\\n",
      "0     AAPL  Apple Could Announce a New iPhone Tomorrow. It...   \n",
      "1     AAPL  Should You Buy Apple Stock Hand Over Fist Befo...   \n",
      "2     AAPL  Apple's Launch of iPhone SE4 Not Seen Impactin...   \n",
      "3     AAPL  Apple Partners With Alibaba, Eyes Baidu for AI...   \n",
      "4     AAPL  Apple Inc. (AAPL) Teases New Product Launch wi...   \n",
      "..     ...                                                ...   \n",
      "133   AMZN  North Carolina Amazon Employees Vote Against U...   \n",
      "134   NVDA  Warren Buffett Winner Soars To Buy Point After...   \n",
      "135    IBM  Data Brokers Market to USD 441.4 Billion by 20...   \n",
      "136   MSFT  This Magnificent Seven Tech Rebounds After Hit...   \n",
      "137   AAPL  'Have Your Own Individual Credit Cards And Che...   \n",
      "\n",
      "                                                  link  similarity  \\\n",
      "0    https://finance.yahoo.com/m/20230221-638c-325c...    0.635073   \n",
      "1    https://finance.yahoo.com/m/6111ff4a-1dcc-3fc2...    0.468467   \n",
      "2    https://finance.yahoo.com/news/apple-apos-laun...    0.444945   \n",
      "3    https://finance.yahoo.com/news/apple-partners-...    0.381238   \n",
      "4    https://finance.yahoo.com/news/apple-inc-aapl-...    0.370252   \n",
      "..                                                 ...         ...   \n",
      "133  https://finance.yahoo.com/m/2e593799-6fea-3b36...    0.093349   \n",
      "134  https://finance.yahoo.com/m/228dd0b4-0ca4-3941...    0.091535   \n",
      "135  https://finance.yahoo.com/news/data-brokers-ma...    0.086088   \n",
      "136  https://finance.yahoo.com/m/80b3aceb-d00b-3b69...    0.082721   \n",
      "137  https://finance.yahoo.com/news/own-individual-...    0.061422   \n",
      "\n",
      "                                             full_text  \n",
      "0    Apple will unveil a product tomorrow, but Wall...  \n",
      "1    Something's cooking in Cupertino. I'm referrin...  \n",
      "2    Apple (AAPL)'s launch of the fourth generation...  \n",
      "3    Apple (AAPL, Financials) partnered with Alibab...  \n",
      "4    We recently compiled a list of the 10 High-Fly...  \n",
      "..                                                 ...  \n",
      "133  North Carolinian Amazon workers voted “no” on ...  \n",
      "134  Bank stocks are near buy points as funds load ...  \n",
      "135  SNS Insider pvt ltd The Data Brokers Market is...  \n",
      "136  When a stock breaks out of a well-formed base,...  \n",
      "137  'Have Your Own Individual Credit Cards And Che...  \n",
      "\n",
      "[138 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "embedding_matrix = np.array(df['embedding'].tolist(), dtype=np.float32)\n",
    "\n",
    "print(f'\\nEmbedding Matrix created from dataframe:\\n{embedding_matrix}')\n",
    "\n",
    "q = np.array(user_in_embedding, dtype=np.float32).reshape(1,-1) # Reshape the matrix to be a 2d Vector with 1 row, and numpy decides the number of columns in this case it is 3072 because that is the embedding array length\n",
    "print(f'\\nOriginal User input query embedding:\\n {np.concatenate([user_in_embedding[:5], user_in_embedding[-5:]])}')\n",
    "print(f'\\n2D vector representation of the user input query embedding:\\n{q}')\n",
    "\n",
    "# Cosine Similarity:\n",
    "sims = cosine_similarity(q, embedding_matrix)[0] # 0 flattens the list of similarity scores, one for each article.\n",
    "\n",
    "df = df.assign(similarity=sims) # Add cosine similarity scores as a new column\n",
    "\n",
    "res_df = (\n",
    "    df.sort_values(\"similarity\", ascending=False)\n",
    "      .loc[:, [\"ticker\", \"title\", \"link\", \"similarity\", \"full_text\"]]\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "\n",
    "print(f'Cosine Similarity results:\\n {res_df}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai_libraries_env)",
   "language": "python",
   "name": "ai_libraries_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
